
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Rank and dimension &#8212; computational-linear-algebra</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../_static/loss_surface.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Four fundamental subspaces" href="four-subspaces.html" />
    <link rel="prev" title="Matrix multiplication and norms" href="mm-norms.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/loss_surface.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">computational-linear-algebra</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  FUNDAMENTALS
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="vectors-and-matrices.html">
   Vectors and matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svd.html">
   Singular value decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mm-norms.html">
   Matrix multiplication and norms
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Rank and dimension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="four-subspaces.html">
   Four fundamental subspaces
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notes/rank.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/particle1331/machine-learning-collection"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/particle1331/machine-learning-collection/issues/new?title=Issue%20on%20page%20%2Fnotes/rank.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="rank-and-dimension">
<h1>Rank and dimension<a class="headerlink" href="#rank-and-dimension" title="Permalink to this headline">¶</a></h1>
<br>
<ul class="simple">
<li><p>(4.1) <strong>Rank as dimensionality of information.</strong> The rank of <span class="math notranslate nohighlight">\(\bold A \in \mathbb R^{m \times n}\)</span> is the maximal number of linearly independent columns of <span class="math notranslate nohighlight">\(\bold A.\)</span> It follows that <span class="math notranslate nohighlight">\(0 \leq r \leq \min(m, n).\)</span> Matrix rank has several applications, e.g. <span class="math notranslate nohighlight">\(\bold A^{-1}\)</span> exists for a square matrix whenever it has maximal rank. In applied settings, rank is used in PCA, Factor Analysis, etc. because rank is used to determine how much nonredundant information is contained in <span class="math notranslate nohighlight">\(\bold A.\)</span></p></li>
</ul>
<br>
<ul class="simple">
<li><p>(4.2) <strong>Computing the rank.</strong> How to count the maximal number of linearly independent columns? (1) Row reduction (can be numerically unstable). (2) Best way is to use SVD. The rank of <span class="math notranslate nohighlight">\(\bold A\)</span> is the number <span class="math notranslate nohighlight">\(r\)</span> of nonzero singular values of <span class="math notranslate nohighlight">\(\bold A.\)</span> This is how it’s implemented in MATLAB and NumPy. The SVD is also used for rank estimation. Another way is to count the number of nonzero eigenvalues of <span class="math notranslate nohighlight">\(\bold A\)</span> provided <span class="math notranslate nohighlight">\(\bold A\)</span> has an eigendecomposition. Since this would imply that <span class="math notranslate nohighlight">\(\bold A\)</span> is similar to its matrix of eigenvalues. This is in general not true. Instead, we can count the eigenvalues of <span class="math notranslate nohighlight">\(\bold A^\top \bold A\)</span> or <span class="math notranslate nohighlight">\(\bold A\bold A^\top\)</span> — whichever is smaller — since an eigendecomposition for both matrices always exist. In practice, counting the rank requires setting a threshold below which values which determine rank are treated as zero, e.g. singular values <span class="math notranslate nohighlight">\(\sigma_k\)</span> in the SVD.</p></li>
</ul>
<br>
<ul class="simple">
<li><p>(4.3) <strong>Rank can be difficult to calculate numerically.</strong> For instance if we obtain <span class="math notranslate nohighlight">\(\sigma_k = 10^{-13}\)</span> numerically, is it a real nonzero singular value, or is it zero? In practice, we set thresholds. The choice of threshold can be arbitrary or domain specific, and in general, introduces its own difficulties. Another issue is noise, adding <span class="math notranslate nohighlight">\(\epsilon\bold I\)</span> makes <span class="math notranslate nohighlight">\(\bold A = [[1, 1], [1, 1]]\)</span> rank two.</p></li>
</ul>
<br>
<ul class="simple">
<li><p>(4.4) <strong>Finding a basis for the column space.</strong> We will be particularly interested in finding a subset of the columns of <span class="math notranslate nohighlight">\(\bold A\)</span> that is a basis for <span class="math notranslate nohighlight">\(\mathsf{C}(\bold A).\)</span> The problem in abstract terms is to find a linearly independent subset of a spanning set that spans the space. One can proceed iteratively. Let <span class="math notranslate nohighlight">\(\bold a_1, \ldots, \bold a_n\)</span> be the columns of <span class="math notranslate nohighlight">\(\bold A\)</span>, take the largest <span class="math notranslate nohighlight">\(j\)</span> such that <span class="math notranslate nohighlight">\(\sum_{j=1}^n c_j \bold a_j = \bold 0\)</span> and <span class="math notranslate nohighlight">\(c_j \neq 0.\)</span> We can remove this vector <span class="math notranslate nohighlight">\(\bold a_j\)</span> such that the remaining columns still span <span class="math notranslate nohighlight">\(\mathsf{C}(\bold A).\)</span> Repeat until we get <span class="math notranslate nohighlight">\(r\)</span> columns that is linearly independent, i.e. <span class="math notranslate nohighlight">\(\sum_{j=1}^n c_j \bold a_j = \bold 0\)</span> implies <span class="math notranslate nohighlight">\(c_j = 0\)</span> for all <span class="math notranslate nohighlight">\(j.\)</span> Further removing any vector fails to span the column space, or the column space is zero in the worst case, so we know this algorithm terminates. <br><br>
Another way to construct a basis for <span class="math notranslate nohighlight">\(\mathsf{C}(\bold A)\)</span> is to perform Gaussian elimination along the columns of <span class="math notranslate nohighlight">\(\bold A\)</span> as each step preserves the column space. The resulting pivot columns form a basis for <span class="math notranslate nohighlight">\(\mathsf{C}(\bold A)\)</span> but is not a subset of the columns of <span class="math notranslate nohighlight">\(\bold A.\)</span></p></li>
</ul>
<br>
<ul>
<li><p>(4.5) <strong>Basis and dimension.</strong> Basis vectors are linearly independent vectors that span the vector space. We will be interested in finite-dimensional vector spaces, i.e. spaces that are spanned by finitely many vectors. By the above algorithm, we can always reduce the spanning set to a basis, so that a finite-dimensional vector space always has a (finite) basis. We know that a basis is not unique — in the previous bullet we constructed two bases for <span class="math notranslate nohighlight">\(\mathsf{C}(\bold A).\)</span> However, once a basis <span class="math notranslate nohighlight">\(\bold v_1, \ldots, \bold v_n\)</span> is fixed, every vector <span class="math notranslate nohighlight">\(\bold x\)</span> has a unique representation <span class="math notranslate nohighlight">\((x_1, \ldots, x_n)\)</span> such that <span class="math notranslate nohighlight">\(\bold x = \sum_{i=1}^n x_i \bold v_i.\)</span> We can think of this as a <strong>parametrization</strong> of the space by <span class="math notranslate nohighlight">\(n\)</span> numbers. It is natural to ask whether there exists a more compressed parametrization, i.e. a basis of shorter length. It turns out that the length of any basis of a finite-dimensional vector space have the same length. Thus, we can think of this number as a property of the space which we define to be its <strong>dimension</strong>. This can be proved with the help of the ff. lemma since a basis is simultaneously spanning and linearly independent:</p>
<blockquote>
<div><p>(Finite-dim.) The cardinality of any linearly independent set of vectors is bounded by the cardinality of any spanning set of the vector space.</p>
</div></blockquote>
<br>
<p><strong>Proof.</strong>   The idea for the proof is that we can iteratively exchange vectors in a spanning set with vectors in linearly independent set while preserving the span.
Let <span class="math notranslate nohighlight">\(\bold u_1, \ldots, \bold u_{s}\)</span> be a linearly independent list and <span class="math notranslate nohighlight">\(\bold v_1, \ldots, \bold v_{t}\)</span> be a spanning set in <span class="math notranslate nohighlight">\(V.\)</span> We iteratively update the spanning set keeping it at fixed length <span class="math notranslate nohighlight">\(t.\)</span> Append <span class="math notranslate nohighlight">\(\bold u_1, \bold v_1, \ldots, \bold v_{t}.\)</span> This list is linearly dependent since <span class="math notranslate nohighlight">\(\bold u_1 \in V.\)</span> Possibly reindexing, let <span class="math notranslate nohighlight">\(\bold u_1\)</span> depend on <span class="math notranslate nohighlight">\(\bold v_1\)</span>, then <span class="math notranslate nohighlight">\(\bold u_1, \bold v_2, \ldots, \bold v_{t}\)</span> spans <span class="math notranslate nohighlight">\(V.\)</span> Now append <span class="math notranslate nohighlight">\(\bold u_2, \bold u_1, \bold v_2, \ldots, \bold v_{t}.\)</span> Note that <span class="math notranslate nohighlight">\(\bold u_2\)</span> cannot depend solely on <span class="math notranslate nohighlight">\(\bold u_1\)</span> by linear independence, so that <span class="math notranslate nohighlight">\(\bold u_2\)</span> depends on <span class="math notranslate nohighlight">\(\bold v_2\)</span> possibly reindexing. That is, we know <span class="math notranslate nohighlight">\(c_2 \neq 0\)</span> in the ff. equation:
$<span class="math notranslate nohighlight">\(
\bold u_2 = c_1 \bold u_1 + \sum_{j=2}^t c_j \bold v_j \implies \bold v_2 = \frac{c_1}{c_2} \bold u_1 - \frac{1}{c_2} \bold u_2 + \sum_{j=3}^t \frac{c_j}{c_2} \bold v_j 
\)</span><span class="math notranslate nohighlight">\(
so that \)</span>\bold u_2, \bold u_1, \bold v_3, \ldots, \bold v_{t}<span class="math notranslate nohighlight">\( spans \)</span>V.<span class="math notranslate nohighlight">\( That is, we have exchanged \)</span>\bold u_2<span class="math notranslate nohighlight">\( with \)</span>\bold v_2<span class="math notranslate nohighlight">\( in the spanning set. Repeat this until we get all \)</span>\bold u_j<span class="math notranslate nohighlight">\('s on the left end of the list. This necessarily implies that \)</span>s \leq t<span class="math notranslate nohighlight">\( since we cannot run out of \)</span>\bold v_i<span class="math notranslate nohighlight">\( vectors in the spanning set due to linear independence of the \)</span>\bold u_j<span class="math notranslate nohighlight">\('s. \)</span>\square$</p>
</li>
</ul>
<br>
<ul class="simple">
<li><p>(4.6) <strong>Obtaining a basis by counting.</strong> This example shows how we can use the bound to reason about linearly independent lists/sets. A linearly independent set of length <span class="math notranslate nohighlight">\(r\)</span> where <span class="math notranslate nohighlight">\(r\)</span> is the dimension of the space is necessarily a basis. Otherwise, we can append the vector that is not spanned to get a linearly independent list of size <span class="math notranslate nohighlight">\(r+1.\)</span></p></li>
</ul>
<br>
<ul class="simple">
<li><p>(4.7) <strong>Row rank = column rank.</strong>
Consider a step in Gaussian elimination along the rows of <span class="math notranslate nohighlight">\(\bold A\)</span> resulting in <span class="math notranslate nohighlight">\(\tilde \bold A.\)</span> We know that <span class="math notranslate nohighlight">\(\mathsf{R}(\bold A) = \mathsf{R}(\tilde\bold A).\)</span> So its clear that the row rank remains unchanged.
On the other hand, let’s consider the independence of the columns after a step. We know there are <span class="math notranslate nohighlight">\(r\)</span> basis vectors in the columns of <span class="math notranslate nohighlight">\(\bold A\)</span> and the <span class="math notranslate nohighlight">\(n - r\)</span> non-basis vectors are a linear combination of the <span class="math notranslate nohighlight">\(r\)</span> basis vectors.
WLOG, suppose the first <span class="math notranslate nohighlight">\(r\)</span> columns of <span class="math notranslate nohighlight">\(\bold A\)</span> form the basis for <span class="math notranslate nohighlight">\(\mathsf{C}(\bold A).\)</span> Then, for <span class="math notranslate nohighlight">\(j &gt; r\)</span>, there exist a vector <span class="math notranslate nohighlight">\(\bold x\)</span> such that <span class="math notranslate nohighlight">\(\bold A \bold x = \bold 0\)</span> and <span class="math notranslate nohighlight">\(x_j = -1\)</span> which encode the dependencies. Moreover, the only solution of the homogeneous system such that <span class="math notranslate nohighlight">\(x_j = 0\)</span> for <span class="math notranslate nohighlight">\(j &gt; r\)</span> is <span class="math notranslate nohighlight">\(\bold x = \bold 0.\)</span> Observe that <span class="math notranslate nohighlight">\(\bold A\)</span> and <span class="math notranslate nohighlight">\(\tilde\bold A\)</span> have the same null space as an inductive invariant, so that the index of the basis vectors in the columns of <span class="math notranslate nohighlight">\(\bold A\)</span> are carried over to <span class="math notranslate nohighlight">\(\tilde \bold A.\)</span> It follows that, the column rank of <span class="math notranslate nohighlight">\(\tilde \bold A\)</span> is equal to the column rank of <span class="math notranslate nohighlight">\(\bold A.\)</span> Thus, in every step of Gaussian elimination the column and row ranks are invariant. At the end of the algorithm, with <span class="math notranslate nohighlight">\(r\)</span> pivots remaining, we can read off that <span class="math notranslate nohighlight">\(r\)</span> maximally independent rows and <span class="math notranslate nohighlight">\(r\)</span> maximally independent columns. It follows that the column and row ranks of <span class="math notranslate nohighlight">\(\bold A\)</span> are equal.</p></li>
</ul>
<br>
<ul>
<li><p>(4.8) <strong>Multiplication with an invertible matrix preserves rank.</strong> Suppose <span class="math notranslate nohighlight">\(\bold U\)</span> is invertible, then <span class="math notranslate nohighlight">\(\bold U \bold A \bold x = \bold 0\)</span> if and only if <span class="math notranslate nohighlight">\(\bold A \bold x = \bold 0\)</span> so that <span class="math notranslate nohighlight">\(\bold U \bold A\)</span> and <span class="math notranslate nohighlight">\(\bold A\)</span> have the same null space. Thus, they have the same rank by the rank-nullity theorem. On the other hand,
$<span class="math notranslate nohighlight">\(
\text{rank }(\bold A \bold U) = \text{rank } (\bold U^\top \bold A^\top) = \text{rank }\bold (\bold A^\top) = \text{rank }\bold (\bold A)
\)</span><span class="math notranslate nohighlight">\( 
since \)</span>\bold U^\top<span class="math notranslate nohighlight">\( is invertible and row rank equals column rank. As a corollary, if two matrices \)</span>\bold A<span class="math notranslate nohighlight">\( and \)</span>\bold B<span class="math notranslate nohighlight">\( are similar, i.e. \)</span>\bold A = \bold U \bold B \bold U^{-1}<span class="math notranslate nohighlight">\( for some invertible matrix \)</span>\bold U<span class="math notranslate nohighlight">\(, then they have the same rank. Another corollary is that the number of nonzero singular values of \)</span>\bold A<span class="math notranslate nohighlight">\( is equal to its rank in the decomposition \)</span>\bold A = \bold U \bold \Sigma \bold V^\top<span class="math notranslate nohighlight">\( since \)</span>\bold U<span class="math notranslate nohighlight">\( and \)</span>\bold V^\top$ are both invertible.</p>
<br>
<p><strong>Remark.</strong> This also geometrically makes sense, i.e. automorphism on the input and output spaces. Applying <span class="math notranslate nohighlight">\(\bold U\)</span> to a basis of <span class="math notranslate nohighlight">\(\mathsf C(\bold A)\)</span> results in a basis of the same cardinality. So that <span class="math notranslate nohighlight">\(\mathsf C(\bold U \bold A)\)</span> has the same dimension. On the other hand, transforming the input space by <span class="math notranslate nohighlight">\(\bold U,\)</span> we still get <span class="math notranslate nohighlight">\(\mathbb R^n\)</span> so that <span class="math notranslate nohighlight">\(\mathsf C(\bold A) = \mathsf C (\bold A \bold U).\)</span> Then, we can prove equality of row and column rank by constructing a CR decomposition by means of left and right multiplying elementary matrices which do not change rank, and whose products have independent columns and rows, respectively. <br><br></p>
<p align="center">
<img src="img/CR_decomp.svg" alt="drawing" width="400"/> <br> 
<b>Figure.</b> Visualizing the CR decomposition.
</p>
</li>
</ul>
<br>
<ul class="simple">
<li><p>(4.9) <strong>Generate rank 4 matrix 10x10 matrix randomly by multiplying two randomly generated matrices.</strong> Solution is to multiply 10x4 and 4x10 matrices. Here we assume, reasonably so, that the randomly generated matrices have maximal rank.</p></li>
</ul>
<br>
<ul class="simple">
<li><p>(4.10) <strong>Rank of <span class="math notranslate nohighlight">\(\bold A^\top \bold A\)</span> and <span class="math notranslate nohighlight">\(\bold A \bold A^\top\)</span>.</strong> These are all equal to the rank of <span class="math notranslate nohighlight">\(\bold A.\)</span>
The first equality can be proved using by showing the <span class="math notranslate nohighlight">\(\mathsf{N} (\bold A^\top \bold A) = \mathsf{N}( \bold A),\)</span> and then invoke the rank-nullity theorem. We used this in the proof of SVD to show conclude that rank <span class="math notranslate nohighlight">\(\bold A\)</span> is the number of nonzero singular values of <span class="math notranslate nohighlight">\(\bold A.\)</span> The second equality follows by replacing <span class="math notranslate nohighlight">\(\bold A\)</span> with <span class="math notranslate nohighlight">\(\bold A^\top\)</span> and the fact that row rank equals column rank.<br />
We can also see this from the SVD which gives us <span class="math notranslate nohighlight">\(\bold A \bold A^\top = \bold U \bold \Sigma \bold \Sigma^\top \bold U^\top\)</span> i.e. similar to <span class="math notranslate nohighlight">\(\Sigma \bold \Sigma^\top\)</span> which has <span class="math notranslate nohighlight">\(r = \text{rank }\bold A\)</span> diagonal entries.
Thus, <span class="math notranslate nohighlight">\(\text{rank } \bold A \bold A^\top = \text{rank }\bold A = r.\)</span></p></li>
</ul>
<br>
<ul>
<li><p>(4.11) <strong>Making a matrix full-rank by shifting:</strong> <span class="math notranslate nohighlight">\(\tilde\bold A = \bold A + \lambda \bold I\)</span> where we assume <span class="math notranslate nohighlight">\(\bold A\)</span> is square. This is done for computational stability reasons. Typically the regularization constant <span class="math notranslate nohighlight">\(\lambda\)</span> is less than the experimental noise. For instance, if <span class="math notranslate nohighlight">\(|\lambda| \gg \max |a_{ij}|,\)</span> then <span class="math notranslate nohighlight">\(\tilde \bold A \approx \lambda \bold I\)</span> and <span class="math notranslate nohighlight">\(\bold A\)</span> becomes the noise. An exchange in the Q&amp;A highlights another important issue. Hamzah asks:</p>
<blockquote>
<div><p>So in a previous video in this section, you talked about how a 3 dimensional matrix spanning a 2 dimensional subspace […] really is a rank 2 matrix, BUT if you introduce some noise, it can look like like a rank 3 matrix. […] By adding the identity matrix, aren’t you essentially deliberately adding noise to an existing dataset to artificially boost the rank? Am I correct in interpreting that you can possibly identify features in the boosted rank matrix that may not actually exist in the true dataset, and maybe come up with some weird conclusions? If that is the case wouldn’t it be very dangerous to increase the rank by adding the identity matrix? Would appreciate some clarification. Thank you!</p>
</div></blockquote>
<p>To which Mike answers:</p>
<blockquote>
<div><p>Excellent observation, and good question. Indeed, there is a huge and decades-old literature about exactly this issue – how much “noise” to add to data? In statistics and machine learning, adding noise is usually done as part of regularization. <br><br>
The easy answer is that you want to shift the matrix by as little as possible to avoid changing the data, while still adding enough to make the solutions work. I don’t go into a lot of detail about that in this course, but often, somewhere around 1% of the average eigenvalues of the matrix provides a good balance. <br><br>
Note that this is done for numerical stability reasons, not for theoretical reasons. So the balance is: Do I want to keep my data perfect and get no results, or am I willing to lose a bit of data precision in order to get good results?</p>
</div></blockquote>
</li>
</ul>
<br>
<ul class="simple">
<li><p>(4.12) <strong>Is this vector in the span of this set?</strong> Let <span class="math notranslate nohighlight">\(\bold x \in \mathbb R^m\)</span> be a test vector. Is <span class="math notranslate nohighlight">\(\bold x\)</span> in the span of <span class="math notranslate nohighlight">\(\bold a_1, \ldots, \bold a_n \in \mathbb R^m.\)</span> Let <span class="math notranslate nohighlight">\(\bold A = [\bold a_1, \ldots, \bold a_n]\)</span> with rank <span class="math notranslate nohighlight">\(r.\)</span> The solution is to check whether the rank of <span class="math notranslate nohighlight">\([\bold A | \bold x]\)</span> is equal to the <span class="math notranslate nohighlight">\(r\)</span> (in span) or <span class="math notranslate nohighlight">\(r+1\)</span> (not in span).</p></li>
</ul>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="mm-norms.html" title="previous page">Matrix multiplication and norms</a>
    <a class='right-next' id="next-link" href="four-subspaces.html" title="next page">Four fundamental subspaces</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ron Medina. Powered by <a href="https://jupyterbook.org">Jupyter Book</a>.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>