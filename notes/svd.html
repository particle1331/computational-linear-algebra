
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Singular value decomposition &#8212; computational-linear-algebra</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../_static/loss_surface.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Matrix multiplication and norms" href="mm-norms.html" />
    <link rel="prev" title="Vectors and matrices" href="vectors-and-matrices.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/loss_surface.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">computational-linear-algebra</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  FUNDAMENTALS
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="vectors-and-matrices.html">
   Vectors and matrices
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Singular value decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mm-norms.html">
   Matrix multiplication and norms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rank.html">
   Rank and dimension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="four-subspaces.html">
   Four fundamental subspaces
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notes/svd.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/particle1331/machine-learning-collection"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/particle1331/machine-learning-collection/issues/new?title=Issue%20on%20page%20%2Fnotes/svd.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="singular-value-decomposition">
<h1>Singular value decomposition<a class="headerlink" href="#singular-value-decomposition" title="Permalink to this headline">¶</a></h1>
<br>
<ul class="simple">
<li><p>(2.1) <strong>Geometry of linear operators.</strong> In the code challenge, we saw that a unit circle is mapped by a square matrix <span class="math notranslate nohighlight">\(\bold A\)</span> into an ellipse. It turns out that the effect of a square matrix <span class="math notranslate nohighlight">\(\bold A \in \mathbb R^{2 \times 2}\)</span> as an operator on <span class="math notranslate nohighlight">\(\mathbb R^2\)</span> is to dilate the space outwards in two orthogonal directions (possibly some directions shrinking to zero, but never in a negative direction), then resulting space is rotated twice. To see this, let <span class="math notranslate nohighlight">\(\bold A = \bold U \bold \Sigma \bold V^\top\)</span> be the SVD of <span class="math notranslate nohighlight">\(\bold A\)</span>, then <span class="math notranslate nohighlight">\(\bold A = (\bold U \bold V^\top) (\bold V \bold\Sigma \bold V^\top)\)</span>. The factor <span class="math notranslate nohighlight">\(\bold V \bold\Sigma \bold V^\top\)</span> dilates the space two orthogonal directions defined by the columns of <span class="math notranslate nohighlight">\(\bold V\)</span> while the strength of the dilation is determined by the singular values in the diagonal of <span class="math notranslate nohighlight">\(\bold \Sigma\)</span>.
We can interpret <span class="math notranslate nohighlight">\(\bold V\)</span> and <span class="math notranslate nohighlight">\(\bold V^\top\)</span> as change of basis matrices, i.e. in terms of a sum of projection operators <span class="math notranslate nohighlight">\(\sum_{i=1}^n \sigma_i \bold v_i \bold v_i^\top\)</span>. This is followed by a product <span class="math notranslate nohighlight">\(\bold U \bold V^\top\)</span> of two isometries of <span class="math notranslate nohighlight">\(\mathbb R^2\)</span>. It can be <a class="reference external" href="https://math.stackexchange.com/a/2924263">easily calculated</a> that orthogonal transformations of <span class="math notranslate nohighlight">\(\mathbb R^2\)</span> are either rotations or reflections, so that we get a final ellipse. Since the rank of <span class="math notranslate nohighlight">\(\bold A\)</span> is equal to the number of nonzero singular values, whenever <span class="math notranslate nohighlight">\(\bold A\)</span> is singular, some of its singular values will be zero corresponding to an axis where the ellipse collapses (see figure below).</p></li>
</ul>
<br>
<ul>
<li><p>(2.2) <strong>Polar decomposition.</strong> The decomposition of an operator <span class="math notranslate nohighlight">\(\bold A \in \mathbb R^{n\times n}\)</span> into <span class="math notranslate nohighlight">\(\bold A = \bold Q \bold P\)</span> where <span class="math notranslate nohighlight">\(\bold Q\)</span> is orthogonal and <span class="math notranslate nohighlight">\(\bold P\)</span> is symmetric positive semidefinite is called the <strong>polar decomposition</strong>. Geometrically, we can see that <span class="math notranslate nohighlight">\(\bold P\)</span> should be unique. Indeed, observe that <span class="math notranslate nohighlight">\(\bold P^2 = \bold A^\top \bold A\)</span> and <span class="math notranslate nohighlight">\(\bold A^\top \bold A\)</span> is evidently symmetric positive semidefinite, so it has a unique symmetric positive semidefinite square root <span class="math notranslate nohighlight">\(\sqrt{\bold A^\top \bold A}\)</span> <a class="reference external" href="https://www.math.drexel.edu/~foucart/TeachingFiles/F12/M504Lect7.pdf">[Thm. 3]</a>. Thus, <span class="math notranslate nohighlight">\(\bold P = \bold V \bold \Sigma \bold V^\top = \sqrt{\bold A ^\top \bold A}\)</span> by uniqueness. Note however that the eigenvectors the orthogonal eigendecomposition into need not be unique (e.g. when the kernel of <span class="math notranslate nohighlight">\(\bold A\)</span> is nonzero). For real matrices, the isometries are precisely the orthogonal matrices. Thus, the polar decomposition can be written as <span class="math notranslate nohighlight">\(\bold A = \bold Q \sqrt{\bold A^\top \bold A}\)</span> for some isometry <span class="math notranslate nohighlight">\(\bold Q\)</span>; cf. <a class="reference external" href="https://www.maa.org/sites/default/files/pdf/awards/Axler-Ford-1996.pdf">[Lem. 9.6]</a> which states the polar decomposition in terms of the existence of such an isometry. The matrix <span class="math notranslate nohighlight">\(\bold Q\)</span> is only unique if <span class="math notranslate nohighlight">\(\bold A\)</span> is nonsingular. For instance, if <span class="math notranslate nohighlight">\(\bold A\)</span> is singular, then we can reflect across the axis where the space is collapsed and still get the same transformation. <br></p>
<p><strong>Remark.</strong> The name “polar decomposition” comes from the analogous decomposition of complex numbers as <span class="math notranslate nohighlight">\(z = re^{i\theta}\)</span> in polar coordinates. Here <span class="math notranslate nohighlight">\(r = \sqrt{\bar z z }\)</span> (analogous to <span class="math notranslate nohighlight">\(\sqrt{\bold A^* \bold A}\)</span>) and multiplication by <span class="math notranslate nohighlight">\(e^{i\theta}\)</span> is an isometry of <span class="math notranslate nohighlight">\(\mathbb C\)</span> (analogous to the isometric property of <span class="math notranslate nohighlight">\(\bold Q\)</span>). For complex matrices we consider <span class="math notranslate nohighlight">\(\bold A^*\bold A\)</span> and unitary matrices in the SVD.</p>
</li>
</ul>
<br>
<ul>
<li><p>(2.3) <strong>Computing the polar decomposition.</strong> In <code class="docutils literal notranslate"><span class="pre">src\4_polar_decomp.py</span></code>, we verify the theory by calculating the polar decomposition from <code class="docutils literal notranslate"><span class="pre">u,</span> <span class="pre">s,</span> <span class="pre">vT</span> <span class="pre">=</span> <span class="pre">np.linalg.svd(A)</span></code>. We set <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">=</span> <span class="pre">u</span> <span class="pre">&#64;</span> <span class="pre">vT</span></code> and <code class="docutils literal notranslate"><span class="pre">P</span> <span class="pre">=</span> <span class="pre">vT.T</span> <span class="pre">&#64;</span> <span class="pre">np.diag(s)</span> <span class="pre">&#64;</span> <span class="pre">vT</span></code>. Some singular values are zero for singular <code class="docutils literal notranslate"><span class="pre">A</span></code>  (left) while all are nonzero for nonsingular <code class="docutils literal notranslate"><span class="pre">A</span></code> (right). The eigenvectors of <code class="docutils literal notranslate"><span class="pre">P</span></code> are scaled by the corresponding eigenvalues, then rotated with <code class="docutils literal notranslate"><span class="pre">Q</span></code>. The rotated eigenvectors of <code class="docutils literal notranslate"><span class="pre">P</span></code> lie along the major and minor axis of the ellipse: the directions where the circle is stretched prior to rotation. The code checks out in that the eigenvectors (obtained from SVD) line up nicely along the axes where the circle is elongated in the scatter plot (obtained by plotting the output vectors <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">&#64;</span> <span class="pre">[x,</span> <span class="pre">y]</span></code> where <code class="docutils literal notranslate"><span class="pre">[x,</span> <span class="pre">y]</span></code> is a point on the unit circle).</p>
  <br>
  <p align="center">
  <img src="img/4_polar_decomposition.png" title="drawing" width="600" />
  </p> 
</li>
</ul>
<br>
<ul>
<li><p>(2.4) <strong>SVD Proof.</strong> The SVD states that any real matrix <span class="math notranslate nohighlight">\(\bold A \in \mathbb R^{m \times n}\)</span> can be decomposed as <span class="math notranslate nohighlight">\(\bold A = \bold U \bold \Sigma \bold V^\top\)</span> where <span class="math notranslate nohighlight">\(\bold U \in \mathbb R^{m \times m}\)</span> and <span class="math notranslate nohighlight">\(\bold V \in \mathbb R^{n \times n}\)</span> are orthonogonal matrices and <span class="math notranslate nohighlight">\(\bold\Sigma  \in \mathbb R^{m \times n}\)</span> is a diagonal matrix with nonnegative real numbers on the diagonal. The diagonal entries <span class="math notranslate nohighlight">\(\sigma_i\)</span> of <span class="math notranslate nohighlight">\(\bold \Sigma\)</span> are called the <strong>singular values</strong> of <span class="math notranslate nohighlight">\(\bold A\)</span>. The number <span class="math notranslate nohighlight">\(r\)</span> of nonzero singular values is equal to the rank of <span class="math notranslate nohighlight">\(\bold A\)</span> as we will show shortly.</p>
  <br>
<p>The following proof of the SVD is constructive, i.e. we construct the singular values, and left and right singular vectors of <span class="math notranslate nohighlight">\(\bold A.\)</span>
Let <span class="math notranslate nohighlight">\(r = \text{rank }\bold A\)</span>, then <span class="math notranslate nohighlight">\(r \leq \min(m, n)\)</span>.
Observe that <span class="math notranslate nohighlight">\(\bold A^\top \bold A \in \mathbb R^{n\times n}\)</span> is symmetric positive semidefinite.
It follows that the eigenvalues of <span class="math notranslate nohighlight">\(\bold A^\top \bold A\)</span> are nonnegative.
and that there exists an eigendecomposition <span class="math notranslate nohighlight">\(\bold A^\top \bold A = \bold V \bold \Sigma^2 \bold V^\top\)</span> where <span class="math notranslate nohighlight">\(\bold V\)</span> is an orthogonal matrix and <span class="math notranslate nohighlight">\(\bold \Sigma^2\)</span> is a diagonal matrix of real eigenvalues of <span class="math notranslate nohighlight">\(\bold A^\top \bold A\)</span> <a class="reference external" href="https://www.maa.org/sites/default/files/pdf/awards/Axler-Ford-1996.pdf">[Theorem 8.3]</a>. Here we let <span class="math notranslate nohighlight">\(\sigma_i = \bold \Sigma_{ii}\)</span> such that <span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \ldots \sigma_r &gt; 0\)</span> where <span class="math notranslate nohighlight">\(r = \text{rank }\bold A.\)</span> This comes from <span class="math notranslate nohighlight">\(\text{rank }\bold A ^\top \bold A = \text{rank }\bold A = r,\)</span> and <span class="math notranslate nohighlight">\(\bold A^\top \bold A\)</span> is similar to <span class="math notranslate nohighlight">\(\bold\Sigma^2,\)</span> so that the first <span class="math notranslate nohighlight">\(r\)</span> singular values of <span class="math notranslate nohighlight">\(\bold A\)</span> are nonzero while the rest are zero. The singular values characterize the geometry of <span class="math notranslate nohighlight">\(\bold A\)</span>. For instance if <span class="math notranslate nohighlight">\(0 \leq r &lt; m\)</span>, then the hyperellipse image of <span class="math notranslate nohighlight">\(\bold A\)</span> collapses to have zero volume. The vectors <span class="math notranslate nohighlight">\(\bold v_1, \ldots, \bold v_n\)</span> form an orthonormal basis for <span class="math notranslate nohighlight">\(\mathbb R^n\)</span> which we call <strong>right singular vectors.</strong> Now that we are done with the domain of <span class="math notranslate nohighlight">\(\bold A,\)</span> we proceed to its codomain.</p>
  <br> 
<p>We know <span class="math notranslate nohighlight">\(\bold A \bold v_i\)</span> for <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, n\)</span> span the image of <span class="math notranslate nohighlight">\(\bold A.\)</span> For <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, r\)</span>, it can be shown that <span class="math notranslate nohighlight">\(\lVert \bold A \bold v_i \rVert = \sigma_i.\)</span> Since the first <span class="math notranslate nohighlight">\(r\)</span> singular values are nonzero, we can define unit vectors <span class="math notranslate nohighlight">\(\bold u_i = {\sigma_i}^{-1}\bold A \bold v_i \in \mathbb R^m\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, r.\)</span> These are the <strong>left singular vectors</strong> of <span class="math notranslate nohighlight">\(\bold A.\)</span> It follows that <span class="math notranslate nohighlight">\(\bold A \bold v_i = \sigma_i \bold u_i\)</span> for <span class="math notranslate nohighlight">\(i = 1, \ldots, r\)</span> and <span class="math notranslate nohighlight">\(\bold A \bold v_i = \bold 0\)</span> for <span class="math notranslate nohighlight">\(i &gt; r.\)</span> Observe that the vectors <span class="math notranslate nohighlight">\(\bold u_i\)</span> are orthogonal
$<span class="math notranslate nohighlight">\(
  \bold u_i^\top \bold u_j = \frac{1}{\sigma_i\sigma_j}\bold v_i^\top\bold A^\top \bold A \bold v_j = \frac{1}{\sigma_i\sigma_j}\bold v_i^\top {\sigma_j}^2 \bold v_j = \delta_{ij} \frac{{\sigma_j}^2}{\sigma_i\sigma_j} = \delta_{ij}.
  \)</span>$</p>
<p>Thus, <span class="math notranslate nohighlight">\(\bold u_1, \ldots \bold u_r\)</span> is an orthonormal basis for the image of <span class="math notranslate nohighlight">\(\bold A\)</span> in <span class="math notranslate nohighlight">\(\mathbb R^m.\)</span> From here we can already obtain the <strong>compact SVD</strong> which already contains all necessary information: <span class="math notranslate nohighlight">\(\bold A = \sum_{k=1}^r \sigma_k \bold  u_k \bold v_k^\top\)</span> or <span class="math notranslate nohighlight">\(\bold A = \bold U_r \bold \Sigma_r \bold V_r^\top\)</span> (bottom of Figure 10.1 below with <span class="math notranslate nohighlight">\(r = n\)</span>) where <span class="math notranslate nohighlight">\(\bold U_r = [\bold u_1, \ldots, \bold u_r] \in \mathbb R^{m \times r},\)</span> <span class="math notranslate nohighlight">\(\bold\Sigma_r = \bold\Sigma[:r, :r],\)</span> and <span class="math notranslate nohighlight">\(\bold V^\top_r = \bold V[:, :r]^\top.\)</span> To get the <strong>full SVD</strong>, we extend <span class="math notranslate nohighlight">\(\bold U_r\)</span> to an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb R^m\)</span> by Gram-Schmidt obtaining <span class="math notranslate nohighlight">\(\bold U = [\bold U_r | \bold U_{m-r}] \in \mathbb R^{m \times m}.\)</span> For <span class="math notranslate nohighlight">\(\bold\Sigma\)</span>, we either pad (<span class="math notranslate nohighlight">\(m &gt; n\)</span>) or remove zero rows (<span class="math notranslate nohighlight">\(m &lt; n\)</span>) to get an <span class="math notranslate nohighlight">\(m \times n\)</span> diagonal matrix. Finally, with these matrices, we can write <span class="math notranslate nohighlight">\(\bold A \bold V = \bold U \bold \Sigma\)</span> so that <span class="math notranslate nohighlight">\(\bold A = \bold U \bold \Sigma \bold V^\top\)</span> where the factors have the properties stated in the SVD. And we’re done! <span class="math notranslate nohighlight">\(\square\)</span></p>
</li>
</ul>
<br>
  <p align="center">
  <img src="img/svd.png" alt="drawing" width="400"/>
  </p>
<br>
<br>
<ul>
<li><p>(2.5) See <code class="docutils literal notranslate"><span class="pre">src/4_svd_from_scratch.py</span></code> for a construction of the (compact) SVD in code following the proof. The result looks great:
<br></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="o">=</span>
<span class="p">[[</span> <span class="mf">1.7641</span>  <span class="mf">0.4002</span>  <span class="mf">0.9787</span>  <span class="mf">2.2409</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">1.8676</span> <span class="o">-</span><span class="mf">0.9773</span>  <span class="mf">0.9501</span> <span class="o">-</span><span class="mf">0.1514</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.1032</span>  <span class="mf">0.4106</span>  <span class="mf">0.144</span>   <span class="mf">1.4543</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.761</span>   <span class="mf">0.1217</span>  <span class="mf">0.4439</span>  <span class="mf">0.3337</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">1.4941</span> <span class="o">-</span><span class="mf">0.2052</span>  <span class="mf">0.3131</span> <span class="o">-</span><span class="mf">0.8541</span><span class="p">]]</span>

<span class="n">U</span> <span class="o">@</span> <span class="n">Sigma</span> <span class="o">@</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span>
<span class="p">[[</span> <span class="mf">1.7641</span>  <span class="mf">0.4002</span>  <span class="mf">0.9787</span>  <span class="mf">2.2409</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">1.8676</span> <span class="o">-</span><span class="mf">0.9773</span>  <span class="mf">0.9501</span> <span class="o">-</span><span class="mf">0.1514</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.1032</span>  <span class="mf">0.4106</span>  <span class="mf">0.144</span>   <span class="mf">1.4543</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.761</span>   <span class="mf">0.1217</span>  <span class="mf">0.4439</span>  <span class="mf">0.3337</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">1.4941</span> <span class="o">-</span><span class="mf">0.2052</span>  <span class="mf">0.3131</span> <span class="o">-</span><span class="mf">0.8541</span><span class="p">]]</span>

<span class="n">Frobenius</span> <span class="n">norms</span><span class="p">:</span>
<span class="o">||</span> <span class="n">A</span> <span class="o">-</span> <span class="n">U</span> <span class="o">@</span> <span class="n">Sigma</span> <span class="o">@</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span> <span class="o">||</span> <span class="o">=</span> <span class="mf">2.371802853223825e-15</span>
<span class="o">||</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">V</span> <span class="o">-</span> <span class="n">I</span> <span class="o">||</span>         <span class="o">=</span> <span class="mf">3.642425835603599e-15</span>
<span class="o">||</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">U</span> <span class="o">-</span> <span class="n">I</span> <span class="o">||</span>         <span class="o">=</span> <span class="mf">2.230019691426858e-14</span>
</pre></div>
</div>
</li>
</ul>
<br>
<ul>
<li><p>(2.6) <strong>Singular vectors in the SVD.</strong> Given the SVD we can write <span class="math notranslate nohighlight">\(\bold A = \sum_{i=1}^r \sigma_i \bold u_i \bold v_i^\top\)</span> as a sum of rank one (!) terms. Recall that <span class="math notranslate nohighlight">\(\sigma_i \bold u_i = \bold A \bold v_i\)</span>. Writing <span class="math notranslate nohighlight">\(\bold A = \sum_{i=1}^r (\bold A \bold v_i) \bold v_i^\top\)</span> is trivial given an ONB <span class="math notranslate nohighlight">\(\bold v_1, \ldots, \bold v_n\)</span> of <span class="math notranslate nohighlight">\(\mathbb R^n.\)</span> What is nontrivial in the SVD is that (1) an ONB always exists that is “natural” to <span class="math notranslate nohighlight">\(\bold A\)</span>, and (2) the corresponding image vectors <span class="math notranslate nohighlight">\(\bold A \bold v_i\)</span> which span <span class="math notranslate nohighlight">\(\textsf{col }\bold A\)</span> are also orthogonal in <span class="math notranslate nohighlight">\(\mathbb R^m.\)</span>
<br></p>
<p align="center">
<img src="img/svd_ellipse.png" alt="Source: http://gregorygundersen.com/image/svd/ellipse.png" width="400"/>
</p>
<br><br>
<p>Another important characterization of the singular vectors is in terms of eigenvalues of <span class="math notranslate nohighlight">\(\bold A^\top \bold A\)</span> and <span class="math notranslate nohighlight">\(\bold A \bold A^\top.\)</span> By construction, <span class="math notranslate nohighlight">\(\bold v_1, \ldots, \bold v_n\)</span> are eigenvectors of <span class="math notranslate nohighlight">\(\bold A^\top \bold A\)</span> with respect to eigenvalues <span class="math notranslate nohighlight">\({\sigma_1}^2, \ldots {\sigma_r}^2, 0, \ldots, 0.\)</span> On the other hand,</p>
<div class="math notranslate nohighlight">
\[\bold A \bold A^\top \bold u_i = \frac{1}{\sigma_i} \bold A \bold A^\top \bold A \bold v_i = \frac{1}{\sigma_i} {\sigma_i}^2 \bold A \bold v_i = {\sigma_i}^2 \bold u_i\]</div>
<p>for <span class="math notranslate nohighlight">\(i = 1, \ldots, r.\)</span> This is also trivially true for <span class="math notranslate nohighlight">\(i &gt; r.\)</span> Thus, <span class="math notranslate nohighlight">\(\bold u_1, \ldots, \bold u_m\)</span> are <span class="math notranslate nohighlight">\(m\)</span> orthogonal eigenvectors of <span class="math notranslate nohighlight">\(\bold A \bold A^\top\)</span> w.r.t. eigenvalues <span class="math notranslate nohighlight">\({\sigma_1}^2, \ldots {\sigma_r}^2, 0, \ldots, 0\)</span>.</p>
<br>
<p align="center">
<img src="img/svd_change_of_basis.svg" alt="drawing" width="400"/> <br> 
<b>Figure. </b> SVD as diagonalization.
</p>
</li>
</ul>
<br>
<ul>
<li><p>(2.7) <strong>SVD as diagonalization.</strong> We can think of the SVD as a change of basis so that the <span class="math notranslate nohighlight">\(m \times n\)</span> matrix <span class="math notranslate nohighlight">\(\bold A\)</span> has a diagonal representation (see Figure above). Recall that we recover the components of a vector in an ONB by performing projection, so we can replace inverses with transpose. In action: <span class="math notranslate nohighlight">\(\bold A = \bold U \bold U^\top \bold A \bold V \bold V^\top = \bold U \bold \Sigma \bold V^\top.\)</span> Here <span class="math notranslate nohighlight">\(\bold U \bold U^\top = \sum_{i = 1}^m \bold u_i \bold {u_i}^\top\)</span> is the change of basis of output vectors of <span class="math notranslate nohighlight">\(\bold \Sigma\)</span> defined by the columns of <span class="math notranslate nohighlight">\(\bold U\)</span> and, similarly, <span class="math notranslate nohighlight">\(\bold V \bold V^\top = \sum_{j = 1}^m \bold v_j \bold {v_j}^\top\)</span> is the change of basis of input vectors of <span class="math notranslate nohighlight">\(\bold \Sigma\)</span> defined by ONB of <span class="math notranslate nohighlight">\(\mathbb R^n\)</span> that form the columns of <span class="math notranslate nohighlight">\(\bold V.\)</span> Thus, the SVD is analogous to diagonalization for square matrices, but instead of eigenvalues, we diagonalize into an <span class="math notranslate nohighlight">\(m \times n\)</span> diagonal matrix of singular values. From <a class="reference external" href="https://www.mathworks.com/content/dam/mathworks/mathworks-dot-com/moler/eigs.pdf">Chapter 10</a> of [Moler, 2013]:</p>
<blockquote>
<div><p>In abstract linear algebra terms, eigenvalues are relevant if a square, <span class="math notranslate nohighlight">\(n\)</span>-by-<span class="math notranslate nohighlight">\(n\)</span> matrix <span class="math notranslate nohighlight">\(\bold A\)</span> is thought of as mapping <span class="math notranslate nohighlight">\(n\)</span>-dimensional space onto itself. We try to find a basis for the space so that the matrix becomes diagonal. This basis might be complex even if <span class="math notranslate nohighlight">\(\bold A\)</span> is real. In fact, if the eigenvectors are not linearly independent, such a basis does not even exist. The SVD is relevant if a possibly rectangular, <span class="math notranslate nohighlight">\(m\)</span>-by-<span class="math notranslate nohighlight">\(n\)</span> matrix <span class="math notranslate nohighlight">\(\bold A\)</span> is thought of as mapping <span class="math notranslate nohighlight">\(n\)</span>-space onto <span class="math notranslate nohighlight">\(m\)</span>-space. We try to find one change of basis in the domain and a usually different change of basis in the range so that the matrix becomes diagonal. Such bases always exist and are always real if <span class="math notranslate nohighlight">\(\bold A\)</span> is real. In fact, the transforming matrices are orthogonal or unitary, so they preserve lengths and angles and do not magnify errors.</p>
</div></blockquote>
</li>
</ul>
<br>
<ul>
<li><p>(2.8) <strong>Computing the SVD.</strong> In <code class="docutils literal notranslate"><span class="pre">4_compute_svd.py</span></code>, we calculate 3 things for a random matrix <span class="math notranslate nohighlight">\(\bold A[i, j] \sim \mathcal{N}(0, 1)\)</span>: (1) equality between the eigenvalues of <span class="math notranslate nohighlight">\(\sqrt{\bold A^\top \bold A}\)</span> and the singular values of <span class="math notranslate nohighlight">\(\bold A\)</span>; (2) difference bet. max. singular value <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\max_{\lVert \bold x \rVert_2 = 1} \lVert \bold A \bold x \rVert_2\)</span>; and (3) whether <span class="math notranslate nohighlight">\(\bold A\bold v_i = \sigma_i \bold u_i\)</span> for <span class="math notranslate nohighlight">\(i = 1, 2\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>λ(√AᵀA):  [2.75276951 1.29375301]
σ(A):     [2.75276951 1.29375301]

| Av - σu |.max()   = 2.220446049250313e-16
σ₁ - max ‖Ax‖ / ‖x‖ = 1.6732994501111875e-07
</pre></div>
</div>
<br>
</li>
<li><p>(2.9) <strong>Spectral theorem proof.</strong> The spectral theorem is an extremely beautiful result which one can think of as the SVD for linear operators. In fact, the construction of the SVD relies on a spectral decomposition, i.e. of <span class="math notranslate nohighlight">\(\bold A^\top \bold A\)</span> which is automatically symmetric.
A key property of symmetric matrices used in the proof is that if <span class="math notranslate nohighlight">\(V\)</span> is a subspace, then <span class="math notranslate nohighlight">\(V^\perp\)</span> is invariant under <span class="math notranslate nohighlight">\(\bold A.\)</span> This will allow us to recursively construct the eigenvector directions of <span class="math notranslate nohighlight">\(\bold A.\)</span> The real spectral theorem generalizes to self-adjoint operators on real inner product spaces as in <a class="reference external" href="https://www.maa.org/sites/default/files/pdf/awards/Axler-Ford-1996.pdf">[Theorem 8.3]</a>. <br><br></p>
<blockquote>
<div><p><strong>Theorem.</strong> (Real spectral theorem). Let <span class="math notranslate nohighlight">\(\bold A \in \mathbb R^{n \times n}\)</span> be a symmetric matrix. Then
(1) the eigenvalues of <span class="math notranslate nohighlight">\(\bold A\)</span> are real;
(2) the eigenvectors of <span class="math notranslate nohighlight">\(\bold A\)</span> corresponding to distinct eigenvalues are orthogonal; and
(3) there exists an ONB of <span class="math notranslate nohighlight">\(\mathbb R^n\)</span> of eigenvectors of <span class="math notranslate nohighlight">\(\bold A.\)</span> This allows the diagonalization
<span class="math notranslate nohighlight">\(\bold A = \sum_{k=1}^n \lambda_k \bold v_k {\bold v_k}^\top = \bold V \bold \Lambda \bold V^\top\)</span>
where <span class="math notranslate nohighlight">\(\bold V\)</span> is a real orthogonal matrix of column stacked eigenvectors <span class="math notranslate nohighlight">\(\bold v_1, \ldots, \bold v_n\)</span> and <span class="math notranslate nohighlight">\(\bold \Lambda\)</span> is a real diagonal matrix of eigenvalues <span class="math notranslate nohighlight">\(\lambda_1, \ldots, \lambda_n.\)</span></p>
</div></blockquote>
<br>
<p><strong>Proof.</strong> [Olver, 2018]. We skip (1) and (2). To prove (3), we perform induction on <span class="math notranslate nohighlight">\(n.\)</span> For <span class="math notranslate nohighlight">\(n = 1\)</span>, this is trivially true with <span class="math notranslate nohighlight">\(\bold A = [a]\)</span> and <span class="math notranslate nohighlight">\(\lambda = a \in \mathbb R\)</span> with eigenvector <span class="math notranslate nohighlight">\(1.\)</span> Suppose <span class="math notranslate nohighlight">\(n \geq 2\)</span> and the spectral theorem is true for symmetric matrices in <span class="math notranslate nohighlight">\(\mathbb R^{n-1}.\)</span> By the <a class="reference external" href="https://math.libretexts.org/Bookshelves/Linear_Algebra/Book%3A_Linear_Algebra_(Schilling_Nachtergaele_and_Lankham)/07%3A_Eigenvalues_and_Eigenvectors/7.04%3A_Existence_of_Eigenvalues">Fundamental Theorem of Algebra (FTA)</a>, there exists at least one eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span> of <span class="math notranslate nohighlight">\(\bold A\)</span> which we know to be real. Along with <span class="math notranslate nohighlight">\(\lambda\)</span> comes a nonzero unit eigenvector <span class="math notranslate nohighlight">\(\bold v \in \mathbb R^n.\)</span> Let <span class="math notranslate nohighlight">\(\bold v^\perp\)</span> be the subspace orthogonal to the <span class="math notranslate nohighlight">\(1\)</span>-dimensional subspace spanned by <span class="math notranslate nohighlight">\(\bold v.\)</span> Then, <span class="math notranslate nohighlight">\(\dim (\bold v^\perp) = n-1\)</span> so that <span class="math notranslate nohighlight">\(\bold v^\perp\)</span> has an orthonormal basis <span class="math notranslate nohighlight">\(\bold y_1, \ldots, \bold y_{n-1} \in \mathbb R^n.\)</span> Moreover, <span class="math notranslate nohighlight">\(\bold v^\perp\)</span> is invariant under <span class="math notranslate nohighlight">\(\bold A\)</span> as a consequence of symmetry.
Suppose <span class="math notranslate nohighlight">\(\bold w \in \bold v^\perp,\)</span> then
$<span class="math notranslate nohighlight">\(
\begin{aligned}
(\bold A \bold w)^\top \bold v 
&amp;= \bold w ^\top \bold A^\top \bold v  \\
&amp;= \bold w ^\top \bold A \bold v \\
&amp;= \lambda \bold w ^\top \bold v = 0.
\end{aligned}
\)</span><span class="math notranslate nohighlight">\(
That is, \)</span>\bold A \bold w \in \bold v^\perp.<span class="math notranslate nohighlight">\( It follows that the restriction \)</span>{\bold A}{|_ {\bold v^\perp}}<span class="math notranslate nohighlight">\( of \)</span>\bold A<span class="math notranslate nohighlight">\( on \)</span>\bold v^\perp<span class="math notranslate nohighlight">\( is well-defined and we can write \)</span>\bold A| _ {\bold v^\perp} = \bold Y \bold B \bold Y^\top<span class="math notranslate nohighlight">\( where \)</span>\bold Y = [\bold y_1, \ldots , \bold y_{n-1}] \in \mathbb R ^{n \times (n-1)}<span class="math notranslate nohighlight">\( and \)</span>\bold B \in \mathbb R^{(n-1) \times (n-1)}<span class="math notranslate nohighlight">\( is the coordinate representation of \)</span>\bold A| _ {\bold v^\perp},<span class="math notranslate nohighlight">\( i.e. \)</span>\bold B = \bold Y^\top \bold A \bold Y.<span class="math notranslate nohighlight">\( Observe that \)</span>\bold B<span class="math notranslate nohighlight">\( is symmetric:
\)</span><span class="math notranslate nohighlight">\(
b_{ij} = {\bold y_i}^\top \bold A \bold y_j = (\bold A^\top \bold y_i)^\top \bold y_j = (\bold A \bold y_i)^\top \bold y_j = b_{ji}.
\)</span><span class="math notranslate nohighlight">\(
By the inductive hypothesis, \)</span>\bold B<span class="math notranslate nohighlight">\( has a spectral decomposition in terms of real eigenvalues \)</span>\omega_1, \ldots, \omega_{n-1}<span class="math notranslate nohighlight">\( and orthonormal eigenvectors \)</span>\bold u_1, \ldots, \bold u_{n-1}<span class="math notranslate nohighlight">\( so that \)</span>\bold B = \bold U \bold \Omega \bold U^\top<span class="math notranslate nohighlight">\( where \)</span>\bold \Omega = \text{diag}(\omega_1, \ldots, \omega_{n-1})<span class="math notranslate nohighlight">\( is a diagonal matrix of real eigenvalues \)</span>\omega_1, \ldots, \omega_{n-1}<span class="math notranslate nohighlight">\( and \)</span>\bold U = [\bold u_1, \ldots, \bold u_{n-1}] \in \mathbb R^{(n-1) \times (n-1)}<span class="math notranslate nohighlight">\( is orthogonal. Thus, 
\)</span><span class="math notranslate nohighlight">\(
{\bold A}{|_ {\bold v^\perp}} = (\bold Y \bold U) \bold \Omega ( \bold Y \bold U)^\top.
\)</span><span class="math notranslate nohighlight">\(
Let \)</span>\bold w_ j = \sum_{k=1}^{n-1} u_{kj} \bold y_k = \bold Y \bold u_j \in \bold v^\perp<span class="math notranslate nohighlight">\( for \)</span>j = 1, \ldots, n-1.<span class="math notranslate nohighlight">\( 
We use the amazing fact that the inner product of vectors \)</span>\bold w_i<span class="math notranslate nohighlight">\( and \)</span>\bold w_j<span class="math notranslate nohighlight">\( represented under an ONB \)</span>\bold y_1, \ldots, \bold y_{n-1}<span class="math notranslate nohighlight">\( reduces to the inner product of its coordinate vectors \)</span>\bold u_i<span class="math notranslate nohighlight">\( and \)</span>\bold u_j<span class="math notranslate nohighlight">\( which are orthonormal by the inductive hypothesis!  That is,
\)</span><span class="math notranslate nohighlight">\(
{\bold w_ i}^\top \bold w_j = {(\bold Y \bold u_i)}^\top {\bold Y \bold u_j} = {\bold u_i}^\top \bold Y ^\top {\bold Y \bold u_j} = \delta_{ij}.
\)</span><span class="math notranslate nohighlight">\(
Hence, \)</span>\bold w_1, \ldots, \bold w_{n-1}<span class="math notranslate nohighlight">\( is an ONB for \)</span>\bold v^\perp.<span class="math notranslate nohighlight">\( Since \)</span>\bold v \perp \bold w_j<span class="math notranslate nohighlight">\( for \)</span>j=1, \ldots, n-1,<span class="math notranslate nohighlight">\( by maximality (1) \)</span>\bold v, \bold w_1 \ldots, \bold w_{n-1}<span class="math notranslate nohighlight">\( is an orthonormal basis of \)</span>\mathbb R^n.<span class="math notranslate nohighlight">\( 
Furthermore, (2) \)</span>\bold A \bold v = \lambda \bold v<span class="math notranslate nohighlight">\( and \)</span>\bold A \bold w_j = \omega_j \bold w_j<span class="math notranslate nohighlight">\( for \)</span>j=1, \ldots, n-1.<span class="math notranslate nohighlight">\( 
These two facts allows us to write
\)</span><span class="math notranslate nohighlight">\(
\begin{aligned}
\bold A
&amp;= \lambda \bold v \bold v^\top + \sum_{j=1}^{n-1}\omega_j \bold w_j{\bold w_j}^\top \\
&amp;= \Bigg[\bold v\; \bold w_1 \ldots \; \bold w_{n-1}\Bigg] \begin{bmatrix}
 \lambda &amp; &amp; \\ 
   &amp; \omega_1 &amp; &amp; \\ 
   &amp;   &amp;  \ddots &amp; \\
   &amp;   &amp;  &amp; \omega_{n-1}
\end{bmatrix}
\begin{bmatrix}
\bold v^\top \\
{\bold w_1}^\top \\ 
\vdots
\\
{\bold w_{n-1}}^\top
\end{bmatrix}.
\end{aligned}
\)</span><span class="math notranslate nohighlight">\(
Observe that (1) allowed a coordinate representation \)</span>\bold A = \bold V \bold \Omega \bold V^\top<span class="math notranslate nohighlight">\( where \)</span>\bold V<span class="math notranslate nohighlight">\( is orthogonal, and (2) guaranteed that \)</span>\bold \Omega<span class="math notranslate nohighlight">\( is diagonal. 
This completes the proof! \)</span>\square$</p>
</li>
</ul>
<br>
<ul>
<li><p>(2.10) <strong>Code demo: spectral theorem proof.</strong> In <code class="docutils literal notranslate"><span class="pre">4_spectral_theorem.py</span></code> we implement the constuction above of an orthonormal eigenbasis for <span class="math notranslate nohighlight">\(\mathbb R^n\)</span> for <span class="math notranslate nohighlight">\(n = 3\)</span> with respect to a randomly generated symmetric matrix <code class="docutils literal notranslate"><span class="pre">A</span></code>. The first eigenvector <span class="math notranslate nohighlight">\(\bold v\)</span> is obtained by cheating a bit, i.e. using <code class="docutils literal notranslate"><span class="pre">np.linalg.eig</span></code>. Then, two linearly independent vectors <span class="math notranslate nohighlight">\(\bold y_1\)</span> and <span class="math notranslate nohighlight">\(\bold y_2\)</span> were constructed by calculating the equation of the plane orthogonal to <span class="math notranslate nohighlight">\(\bold v\)</span> and finding <span class="math notranslate nohighlight">\(x\)</span>’s such that <span class="math notranslate nohighlight">\((x, 1, 1)\)</span> and <span class="math notranslate nohighlight">\((x, 1, 0)\)</span> are points on the plane <span class="math notranslate nohighlight">\(\bold v^\perp.\)</span> Finally, the vectors <span class="math notranslate nohighlight">\(\bold y_1\)</span> and <span class="math notranslate nohighlight">\(\bold y_2\)</span> are made to be orthonormal by Gram-Schmidt. By the inductive hypothesis, we are allowed to compute <code class="docutils literal notranslate"><span class="pre">omega,</span> <span class="pre">U</span> <span class="pre">=</span> <span class="pre">np.linalg.eig(B)</span></code> where <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">=</span> <span class="pre">Y.T</span> <span class="pre">&#64;</span> <span class="pre">A</span> <span class="pre">&#64;</span> <span class="pre">Y</span></code>. Then, we set <code class="docutils literal notranslate"><span class="pre">W</span> <span class="pre">=</span> <span class="pre">Y</span> <span class="pre">&#64;</span> <span class="pre">U</span></code> to be the <span class="math notranslate nohighlight">\(n-1\)</span> eigenvector directions in the orthogonal plane. This is concatenated with <span class="math notranslate nohighlight">\(\bold v\)</span> to get the final matrix <code class="docutils literal notranslate"><span class="pre">V</span></code> of all <span class="math notranslate nohighlight">\(n\)</span> eigenvectors. The eigenvalues are constructed likewise in decreasing order.</p>
<br>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">123</span><span class="p">]:</span> <span class="o">%</span><span class="n">run</span> <span class="mi">4</span><span class="n">_spectral_theorem</span><span class="o">.</span><span class="n">py</span>                                                                                               
<span class="n">A</span> <span class="o">=</span>
<span class="p">[[</span> <span class="mf">9.03615101</span>  <span class="mf">4.74709353</span> <span class="o">-</span><span class="mf">0.56149735</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">4.74709353</span>  <span class="mf">3.67080764</span> <span class="o">-</span><span class="mf">1.41785114</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.56149735</span> <span class="o">-</span><span class="mf">1.41785114</span>  <span class="mf">1.92365423</span><span class="p">]]</span>

<span class="n">B</span> <span class="o">=</span>
<span class="p">[[</span> <span class="mf">0.37139617</span> <span class="o">-</span><span class="mf">0.36034904</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.36034904</span>  <span class="mf">2.30840586</span><span class="p">]]</span>

<span class="n">V</span> <span class="o">=</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.85231143</span>  <span class="mf">0.36198424</span>  <span class="mf">0.37753496</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.50914829</span> <span class="o">-</span><span class="mf">0.40898848</span> <span class="o">-</span><span class="mf">0.75729548</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.11972159</span>  <span class="mf">0.83767287</span> <span class="o">-</span><span class="mf">0.53288921</span><span class="p">]]</span>

<span class="n">V</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">V</span> <span class="o">=</span>
<span class="p">[[</span> <span class="mf">1.00000000e+00</span>  <span class="mf">6.44268216e-17</span> <span class="o">-</span><span class="mf">6.75467825e-17</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">6.44268216e-17</span>  <span class="mf">1.00000000e+00</span> <span class="o">-</span><span class="mf">1.58338343e-16</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">6.75467825e-17</span> <span class="o">-</span><span class="mf">1.58338343e-16</span>  <span class="mf">1.00000000e+00</span><span class="p">]]</span>

<span class="n">Lambda</span> <span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="mf">11.95081085</span>  <span class="mf">2.37327078</span>  <span class="mf">0.30653125</span><span class="p">]</span>
<span class="n">L1</span> <span class="n">error</span> <span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">V</span> <span class="o">@</span> <span class="n">Lambda</span> <span class="o">@</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">=</span> <span class="mf">2.6867397195928788e-14</span>

<span class="n">Compare</span> <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
<span class="p">[</span><span class="mf">11.95081085</span>  <span class="mf">2.37327078</span>  <span class="mf">0.30653125</span><span class="p">]</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.85231143</span> <span class="o">-</span><span class="mf">0.36198424</span> <span class="o">-</span><span class="mf">0.37753496</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.50914829</span>  <span class="mf">0.40898848</span>  <span class="mf">0.75729548</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.11972159</span> <span class="o">-</span><span class="mf">0.83767287</span>  <span class="mf">0.53288921</span><span class="p">]]</span>
</pre></div>
</div>
</li>
</ul>
<br>
<ul>
<li><p>(2.11) <strong>Condition number as measure of stability.</strong> The <strong>condition number</strong> of a matrix is the ratio of its largest to its smallest eigenvalue, i.e. <span class="math notranslate nohighlight">\(\kappa(\bold A) = \dfrac{\sigma_1}{\sigma_r}\)</span> where <span class="math notranslate nohighlight">\(r = \text{rank }\bold A \geq 1.\)</span> Recall that <span class="math notranslate nohighlight">\(\sigma_1\)</span> is the maximum stretching while <span class="math notranslate nohighlight">\(\sigma_r\)</span> gives the minimum for unit vector inputs. Consider <span class="math notranslate nohighlight">\(\bold A \bold x = \bold b\)</span> and a perturbation <span class="math notranslate nohighlight">\(\delta\bold x\)</span> on the input <span class="math notranslate nohighlight">\(\bold x.\)</span> By linearity,
$<span class="math notranslate nohighlight">\(\bold A (\bold x + \delta\bold x) = \bold b + \delta \bold b\)</span>$</p>
<p>where <span class="math notranslate nohighlight">\(\delta\bold b = \bold A (\delta \bold x).\)</span> We know that <span class="math notranslate nohighlight">\(\lVert \bold b \rVert \leq \sigma_1 \lVert \bold x \rVert\)</span> and <span class="math notranslate nohighlight">\(\lVert \delta\bold b \rVert \geq \sigma_r \lVert \delta \bold x \rVert.\)</span> Dividing the right inequality by the left, we preserve the right inequality
$<span class="math notranslate nohighlight">\(\dfrac{\lVert \delta\bold b \rVert}{\lVert \bold b \rVert} \kappa(\bold A) \geq \dfrac{\lVert \delta \bold x \rVert} {\lVert \bold x \rVert}.\)</span>$</p>
<p>Thus, the relative perturbation on the input is bounded by the relative perturbation of the output multiplied by the condition number <span class="math notranslate nohighlight">\(\kappa(\bold A).\)</span> Changes in the right-hand side can cause changes <span class="math notranslate nohighlight">\(\kappa(\bold A)\)</span> times as large in the solution. Note that the quantities on the input and output are dimensionless and scale independent.</p>
<br>
<p align="center">
  <img src='img/13_condition_number_spheres.png' width=60%>
</p>
</li>
</ul>
<br>
<ul class="simple">
<li><p>(2.12) <strong>Scree plots.</strong> Scree plots are plots of singular values of a matrix. These allow us to visualize the relative sizes of singular values. In particular, see which <span class="math notranslate nohighlight">\(k\)</span> singular values are dominant. We will show an example in the next code challenge.</p></li>
</ul>
<br>
<ul>
<li><p>(2.13) <strong>Layer perspective and layer weight.</strong> We can write <span class="math notranslate nohighlight">\(\bold A = \sum_{k=1}^{\min{(m, n)}} \sigma_k \bold u_k \bold v_k^\top.\)</span> Note that since the singular vectors have norm <span class="math notranslate nohighlight">\(1.\)</span> Then, <span class="math notranslate nohighlight">\(\sigma_k\)</span> can be interpreted as the importance of the <span class="math notranslate nohighlight">\(k\)</span>th layer. Most matrices with a definite structure have only a few relatively large singular values with significant values, while most are close to zero. On the other hand, random / noisy matrices have a large number of nonzero singular values. For example, for the image of a dog (<code class="docutils literal notranslate"><span class="pre">13_img_svd.py</span></code>):</p>
<br>
<p align="center">
  <img src='img/dog.jpg' width=60%>
</p>
<p>We construct the first <span class="math notranslate nohighlight">\(k\)</span> layers to make an image. Note that the layers are additive and we can write
$<span class="math notranslate nohighlight">\(\bold A = \sum_{j \leq k} \sigma_k \bold u_k \bold v_k^\top +\sum_{j &gt; k} \sigma_k \bold u_k \bold v_k^\top\)</span>$</p>
<p>to reconstruct the image. In each row below, the left term corresponds to the left image, the right term for the right image. These images sum to the original image (grayscaled). Each rank 1 layer looks the the left image for <span class="math notranslate nohighlight">\(k=1.\)</span></p>
<br>
<p align="center">
  <img src='img/13_img_svd-reconstruction.jpg'>
</p>
<br>
<p>By only using 30 layers, we are able to reconstruct almost all semantically meaningful information content of the image. The rest of the ~1000 layers provides information about the noise as evidenced by the scree plot. In contrast, a random matrix has a scree plot that has almost a linear shape which indicates that there is no semantic meaning in the matrix which manifests as a low-dimensional structure.</p>
<br>
<p align="center">
  <img src='img/13_img_svd-scree.png'>
</p>
</li>
</ul>
<br>
<ul>
<li><p>(2.14) <strong>Code challenge: random matrix with a given condition number.</strong> Construct a random matrix with condition number 42. To do this, construct a linear function <span class="math notranslate nohighlight">\(f(\sigma) = a\sigma + b\)</span> such that <span class="math notranslate nohighlight">\(f(\sigma_1) = 42\)</span> and <span class="math notranslate nohighlight">\(f(\sigma_r) = 1.\)</span> Let <span class="math notranslate nohighlight">\(\bold A\)</span> be a random matrix with SVD <span class="math notranslate nohighlight">\(\bold A = {\bold U \bold \Sigma \bold V}^\top.\)</span> Then, the solution is given by
$<span class="math notranslate nohighlight">\(\bold A_{42} = \bold U \cdot f(\bold \Sigma) \cdot \bold V^\top.\)</span>$</p>
<p>Not sure about uniqueness, but let’s try to plot. Looks okay!</p>
<br>
<p align="center">
  <img src='img/13_kappa=42.png'>
</p>
</li>
</ul>
<br>
<ul>
<li><p>(2.15) <strong>Smooth KDE.</strong> Dog image too large, instead we make an artificial example of a sum of 2D Gaussians to demonstrate the idea of how the relatively low number of layers in the SVD decomposition provide the majority of information in a matrix. The nonzero singular values occupy a small bright streak on the upper left of the middle plot. Moreover, the first few singular vectors look meaningful whereas the rest look more and more like noise — these are the singular vectors that reconstruct most of the meaningful structure in the matrix. This is not the case for the random matrix where there is no low-dimensional or low-rank structure.</p>
<p align="center">
  <img src='img/13_kde.png'>
</p>
</li>
</ul>
<br>
<ul class="simple">
<li><p>(2.16) <strong>Low-dimensional structure.</strong> One feature of the layer perspective is that it reveals the low rank structure of <span class="math notranslate nohighlight">\(\bold A\)</span> in terms of <span class="math notranslate nohighlight">\(\bold A_k = \sum_{j=1}^k \sigma_j \bold u_j \bold v_j^\top\)</span> as a <span class="math notranslate nohighlight">\(k\)</span>-rank approximation of <span class="math notranslate nohighlight">\(\bold A.\)</span> Recall that it can happen that <span class="math notranslate nohighlight">\(k \ll \min(m, n)\)</span> while <span class="math notranslate nohighlight">\(\sum_{j = 1}^k \sigma_j  \approx \sum_{j = 1}^{\min(m, n)}  \sigma_j.\)</span> This was demonstrated above in the dog image example where the sum of the first few layers gives a good approximation to the image. In this case, we say that the image has a low-dimensional structure that we are able to approximate using the first <span class="math notranslate nohighlight">\(k\)</span> layers with the strongest singular values.</p></li>
</ul>
<br>
<ul>
<li><p>(2.17) <strong>Eckart-Young Theorem.</strong> In the above bullet, we discussed the concept of low-rank approximation. Knowing that <span class="math notranslate nohighlight">\(\bold A\)</span> has a low-rank structure from the scree plot, is there a better approximation than the natural <span class="math notranslate nohighlight">\(\bold A_k\)</span>? It turns out that by the Eckart-Young theorem that there is none:</p>
<blockquote>
<div><p>(Eckart-Young). If <span class="math notranslate nohighlight">\(\bold B\)</span> is a rank <span class="math notranslate nohighlight">\(k\)</span> matrix, then <span class="math notranslate nohighlight">\(\lVert \bold A - \bold B \rVert \geq \lVert \bold A - \bold A_k \rVert.\)</span></p>
</div></blockquote>
<p>Note that the norm <span class="math notranslate nohighlight">\(\lVert \cdot \rVert\)</span> used here is the operator norm defined in the next section.</p>
<br>
<p><strong>Proof.</strong> Let <span class="math notranslate nohighlight">\(\bold v_1, \ldots, \bold v_n\)</span> be right singular vectors of <span class="math notranslate nohighlight">\(\bold A.\)</span> Note that <span class="math notranslate nohighlight">\(\dim \mathsf{N}(\bold B) = n-k\)</span> (rank-nullity theorem) and <span class="math notranslate nohighlight">\(\dim \mathsf{C}(\bold v_1, \ldots, \bold v_{k+1}) = k+1.\)</span> Let <span class="math notranslate nohighlight">\(\bold u_1, \ldots, \bold u_{n-k}\)</span> be a basis of <span class="math notranslate nohighlight">\(\mathsf{N}(\bold B).\)</span> So the dimensions of the two subspaces sum to <span class="math notranslate nohighlight">\(n + 1.\)</span> It follows that there exists <span class="math notranslate nohighlight">\(j\)</span> such that <span class="math notranslate nohighlight">\(\bold u_j = a_1\bold u_1 + \ldots a_{j-1}\bold u_{j-1} + \sum_{i=1}^{k+1}c_i\bold v_i\)</span> where <span class="math notranslate nohighlight">\(c_i\)</span> are not all zero. Otherwise, <span class="math notranslate nohighlight">\(\bold u_j\)</span> is a linear combination of the earlier vectors. Let <span class="math notranslate nohighlight">\(\bold u = \bold u_j - \sum_{l=1}^{j-1}c_l\bold u_l.\)</span> Thus, <span class="math notranslate nohighlight">\(\bold u = \sum_{j=1}^{k+1} c_i \bold v_i\)</span> and <span class="math notranslate nohighlight">\(\bold B \bold u = \bold 0.\)</span> Rescale the coefficients so that <span class="math notranslate nohighlight">\(\bold u\)</span> is a unit vector. Then,
$<span class="math notranslate nohighlight">\(
  \|\bold A-\bold B\|^{2} \geq\|(\bold A-\bold B) \bold u\|^{2}=\|\bold A \bold u \|^{2}=\sum_{i=1}^{k+1} {c_i}^2 {\sigma_{i}}^{2} \geq \sigma_{k+1}^{2} \sum_{i=1}^{k+1}{c_i}^2 = \sigma_{k+1}^{2}.
  \)</span>$</p>
<p>We know that <span class="math notranslate nohighlight">\(\|\bold A-\bold A_k\| = \sigma_{k+1}\)</span> since this is just the matrix obtained by replacing the first singular values by zero, i.e. flattening the first <span class="math notranslate nohighlight">\(k\)</span> axes of the ellipse. It follows that <span class="math notranslate nohighlight">\(\|\bold A-\bold B\| \geq \|\bold A-\bold A_k\|.\)</span> <span class="math notranslate nohighlight">\(\square\)</span></p>
</li>
</ul>
<br></div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="vectors-and-matrices.html" title="previous page">Vectors and matrices</a>
    <a class='right-next' id="next-link" href="mm-norms.html" title="next page">Matrix multiplication and norms</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ron Medina. Powered by <a href="https://jupyterbook.org">Jupyter Book</a>.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>